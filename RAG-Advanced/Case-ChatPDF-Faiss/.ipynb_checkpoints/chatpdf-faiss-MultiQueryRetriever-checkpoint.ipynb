{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6750ec-c305-4004-9a9a-09664aa59813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现现有向量数据库: ./vector_db\n",
      "向量数据库已从 ./vector_db 加载。\n",
      "页码信息已加载。\n",
      "\n",
      "==================================================\n",
      "查询: 客户经理被投诉了，投诉一次扣多少分\n",
      "找到 4 个相关文档\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_65280\\2259914909.py:195: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询已处理。成本: Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 0\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "\n",
      "回答:\n",
      "根据提供的信息，如果客户经理被客户投诉一次，每次投诉会扣2分。具体参考如下：\n",
      "\n",
      "**服务质量考核** 中提到：\n",
      "> \"客户服务效率低，态度生硬或不及时为客户提供维护服务，有客户投诉的, 每投诉一次扣 2分\"\n",
      "\n",
      "来源页码:\n",
      "- 第 1 页\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "查询: 客户经理每年评聘申报时间是怎样的？\n",
      "找到 5 个相关文档\n",
      "查询已处理。成本: Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 0\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "\n",
      "回答:\n",
      "客户经理每年评聘申报时间为每年一月份。由分行人力资源部、个人业务部在每年二月份组织统一的资格考试。考试合格者由分行颁发个金客户经理资格证书，其有效期为一年。\n",
      "\n",
      "来源页码:\n",
      "- 第 1 页\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "查询: 客户经理的考核标准是什么？\n",
      "找到 4 个相关文档\n",
      "查询已处理。成本: Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 0\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "\n",
      "回答:\n",
      "客户经理的考核标准主要包括个人业绩考核和工作质量考核两大类：\n",
      "\n",
      "1. **个人业绩考核**：\n",
      "   - 个金客户经理个人业绩以储蓄季日均、季有效净增发卡量、季净增个贷余额三项业务为主要考核指标，实行季度考核。\n",
      "   - 具体标准如下：\n",
      "     - 储蓄业务（季日均余额）为各类个金客户经理考核进入的最低标准。\n",
      "     - 卡业务（季新增发有效卡量）为见习、D类、初级客户经理进入的最低标准。\n",
      "     - 个贷业务（季新增发放个贷）为中级以上客户经理考核进入的最低标准。\n",
      "   - 超出最低考核标准可相互折算，折算标准：50万储蓄 = 50万个贷 = 50张有效卡 = 5分（折算以5分为单位）。\n",
      "\n",
      "2. **工作质量考核**：\n",
      "   - 工作质量考核实行扣分制。工作质量指个金客户经理在从事所有个人业务时出现投诉、差错及风险。\n",
      "   - 服务质量考核包括：\n",
      "     - 工作责任心不强，缺乏配合协作精神；扣5分。\n",
      "     - 客户服务效率低，态度生硬或不及时为客户提供维护服务，有客户投诉的，每投诉一次扣2分。\n",
      "     - 不服从支行工作安排，不认真参加分（支）行宣传活动的，每次扣2分。\n",
      "     - 未能及时参加分行（支行）组织的各种业务培训、考试和专题活动的每次扣2分。\n",
      "     - 未按规定要求进行贷前调查、贷后检查工作的，每笔扣5分。\n",
      "     - 未建立信贷台帐资料及档案的每笔扣5分。\n",
      "     - 在工作中有不廉洁自律情况的每发现一次扣50分。\n",
      "   - 个人资产质量考核：\n",
      "     - 当季考核收息率97%以上为合格，每降1个百分点扣2分。\n",
      "     - 不良资产零为合格，每超一个个百分点扣1分。\n",
      "     - 发生跨月逾期，单笔不超过10万元，当季收回者，扣1分。\n",
      "     - 发生跨月逾期，2笔以上累计金额不超过20万元，当季收回者，扣2分；累计超过20万元以上的，扣4分。\n",
      "     - 发生逾期超过3个月，无论金额大小和笔数，扣10分。\n",
      "\n",
      "这些考核标准旨在全面评估客户经理的工作表现和个人业绩。\n",
      "\n",
      "来源页码:\n",
      "- 第 1 页\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain_community.llms import Tongyi\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import pickle\n",
    "from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "\n",
    "# 获取环境变量中的 DASHSCOPE_API_KEY\n",
    "DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    raise ValueError(\"请设置环境变量 DASHSCOPE_API_KEY\")\n",
    "\n",
    "def extract_text_with_page_numbers(pdf) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    从PDF中提取文本并记录每行文本对应的页码\n",
    "    \n",
    "    参数:\n",
    "        pdf: PDF文件对象\n",
    "    \n",
    "    返回:\n",
    "        text: 提取的文本内容\n",
    "        page_numbers: 每行文本对应的页码列表\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    page_numbers = []\n",
    "\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:\n",
    "            text += extracted_text\n",
    "            page_numbers.extend([page_number] * len(extracted_text.split(\"\\n\")))\n",
    "        else:\n",
    "            print(f\"No text found on page {page_number}.\")\n",
    "\n",
    "    return text, page_numbers\n",
    "\n",
    "def process_text_with_splitter(text: str, page_numbers: List[int], save_path: str = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    处理文本并创建向量存储\n",
    "    \n",
    "    参数:\n",
    "        text: 提取的文本内容\n",
    "        page_numbers: 每行文本对应的页码列表\n",
    "        save_path: 可选，保存向量数据库的路径\n",
    "    \n",
    "    返回:\n",
    "        knowledgeBase: 基于FAISS的向量存储对象\n",
    "    \"\"\"\n",
    "    # 创建文本分割器，用于将长文本分割成小块\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    # 分割文本\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"文本被分割成 {len(chunks)} 个块。\")\n",
    "        \n",
    "    # 创建嵌入模型\n",
    "    embeddings = DashScopeEmbeddings(\n",
    "        model=\"text-embedding-v1\",\n",
    "        dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "    )\n",
    "    \n",
    "    # 从文本块创建知识库\n",
    "    knowledgeBase = FAISS.from_texts(chunks, embeddings)\n",
    "    print(\"已从文本块创建知识库。\")\n",
    "    \n",
    "    # 改进：存储每个文本块对应的页码信息\n",
    "    lines = text.split(\"\\n\")\n",
    "    page_info = {}\n",
    "    for chunk in chunks:\n",
    "        # 查找chunk在原始文本中的开始位置\n",
    "        start_idx = text.find(chunk[:100])  # 使用chunk的前100个字符作为定位点\n",
    "        if start_idx == -1:\n",
    "            # 如果找不到精确匹配，则使用模糊匹配\n",
    "            for i, line in enumerate(lines):\n",
    "                if chunk.startswith(line[:min(50, len(line))]):\n",
    "                    start_idx = i\n",
    "                    break\n",
    "            if start_idx == -1:\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line and line in chunk:\n",
    "                        start_idx = text.find(line)\n",
    "                        break\n",
    "        if start_idx != -1:\n",
    "            line_count = text[:start_idx].count(\"\\n\")\n",
    "            if line_count < len(page_numbers):\n",
    "                page_info[chunk] = page_numbers[line_count]\n",
    "            else:\n",
    "                page_info[chunk] = page_numbers[-1] if page_numbers else 1\n",
    "        else:\n",
    "            page_info[chunk] = -1\n",
    "    knowledgeBase.page_info = page_info\n",
    "    \n",
    "    # 如果提供了保存路径，则保存向量数据库和页码信息\n",
    "    if save_path:\n",
    "        # 确保目录存在\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # 保存FAISS向量数据库\n",
    "        knowledgeBase.save_local(save_path)\n",
    "        print(f\"向量数据库已保存到: {save_path}\")\n",
    "        \n",
    "        # 保存页码信息到同一目录\n",
    "        with open(os.path.join(save_path, \"page_info.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(page_info, f)\n",
    "        print(f\"页码信息已保存到: {os.path.join(save_path, 'page_info.pkl')}\")\n",
    "\n",
    "    return knowledgeBase\n",
    "\n",
    "def load_knowledge_base(load_path: str, embeddings = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    从磁盘加载向量数据库和页码信息\n",
    "    \n",
    "    参数:\n",
    "        load_path: 向量数据库的保存路径\n",
    "        embeddings: 可选，嵌入模型。如果为None，将创建一个新的DashScopeEmbeddings实例\n",
    "    \n",
    "    返回:\n",
    "        knowledgeBase: 加载的FAISS向量数据库对象\n",
    "    \"\"\"\n",
    "    # 如果没有提供嵌入模型，则创建一个新的\n",
    "    if embeddings is None:\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "        )\n",
    "    \n",
    "    # 加载FAISS向量数据库，添加allow_dangerous_deserialization=True参数以允许反序列化\n",
    "    knowledgeBase = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(f\"向量数据库已从 {load_path} 加载。\")\n",
    "    \n",
    "    # 加载页码信息\n",
    "    page_info_path = os.path.join(load_path, \"page_info.pkl\")\n",
    "    if os.path.exists(page_info_path):\n",
    "        with open(page_info_path, \"rb\") as f:\n",
    "            page_info = pickle.load(f)\n",
    "        knowledgeBase.page_info = page_info\n",
    "        print(\"页码信息已加载。\")\n",
    "    else:\n",
    "        print(\"警告: 未找到页码信息文件。\")\n",
    "    \n",
    "    return knowledgeBase\n",
    "\n",
    "def create_multi_query_retriever(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    创建MultiQueryRetriever\n",
    "    \n",
    "    参数:\n",
    "        vectorstore: 向量数据库\n",
    "        llm: 大语言模型，用于查询改写\n",
    "    \n",
    "    返回:\n",
    "        retriever: MultiQueryRetriever对象\n",
    "    \"\"\"\n",
    "    # 创建基础检索器\n",
    "    base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # 创建MultiQueryRetriever\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=base_retriever,\n",
    "        llm=llm\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "def process_query_with_multi_retriever(query: str, retriever, llm):\n",
    "    \"\"\"\n",
    "    使用MultiQueryRetriever处理查询\n",
    "    \n",
    "    参数:\n",
    "        query: 用户查询\n",
    "        retriever: MultiQueryRetriever对象\n",
    "        llm: 大语言模型\n",
    "    \n",
    "    返回:\n",
    "        response: 回答\n",
    "        unique_pages: 相关文档的页码集合\n",
    "    \"\"\"\n",
    "    # 执行查询，获取相关文档\n",
    "    docs = retriever.invoke(query)\n",
    "    print(f\"找到 {len(docs)} 个相关文档\")\n",
    "    \n",
    "    # 加载问答链\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    \n",
    "    # 准备输入数据\n",
    "    input_data = {\"input_documents\": docs, \"question\": query}\n",
    "    \n",
    "    # 使用回调函数跟踪API调用成本\n",
    "    with get_openai_callback() as cost:\n",
    "        # 执行问答链\n",
    "        response = chain.invoke(input=input_data)\n",
    "        print(f\"查询已处理。成本: {cost}\")\n",
    "    \n",
    "    # 记录唯一的页码\n",
    "    unique_pages = set()\n",
    "    \n",
    "    # 获取每个文档块的来源页码\n",
    "    for doc in docs:\n",
    "        text_content = getattr(doc, \"page_content\", \"\")\n",
    "        # 获取向量存储中的页码信息\n",
    "        source_page = retriever.retriever.vectorstore.page_info.get(\n",
    "            text_content.strip(), \"未知\"\n",
    "        )\n",
    "        \n",
    "        if source_page not in unique_pages:\n",
    "            unique_pages.add(source_page)\n",
    "    \n",
    "    return response, unique_pages\n",
    "\n",
    "\n",
    "# 设置PDF文件路径\n",
    "pdf_path = './浦发上海浦东发展银行西安分行个金客户经理考核办法.pdf'\n",
    "# 设置向量数据库保存路径\n",
    "vector_db_path = './vector_db'\n",
    "\n",
    "# 检查向量数据库是否已存在\n",
    "if os.path.exists(vector_db_path) and os.path.isdir(vector_db_path):\n",
    "    print(f\"发现现有向量数据库: {vector_db_path}\")\n",
    "    # 创建嵌入模型\n",
    "    embeddings = DashScopeEmbeddings(\n",
    "        model=\"text-embedding-v1\",\n",
    "        dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "    )\n",
    "    # 加载向量数据库\n",
    "    knowledgeBase = load_knowledge_base(vector_db_path, embeddings)\n",
    "else:\n",
    "    print(f\"未找到向量数据库，将从PDF创建新的向量数据库\")\n",
    "    # 读取PDF文件\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    # 提取文本和页码信息\n",
    "    text, page_numbers = extract_text_with_page_numbers(pdf_reader)\n",
    "    print(f\"提取的文本长度: {len(text)} 个字符。\")\n",
    "    \n",
    "    # 处理文本并创建知识库，同时保存到磁盘\n",
    "    knowledgeBase = process_text_with_splitter(text, page_numbers, save_path=vector_db_path)\n",
    "\n",
    "# 初始化大语言模型（用于查询改写和回答生成）\n",
    "llm = Tongyi(model_name=\"deepseek-v3\", dashscope_api_key=DASHSCOPE_API_KEY)\n",
    "\n",
    "# 创建MultiQueryRetriever\n",
    "multi_retriever = create_multi_query_retriever(knowledgeBase, llm)\n",
    "\n",
    "# 设置查询问题\n",
    "queries = [\n",
    "    \"客户经理被投诉了，投诉一次扣多少分\",\n",
    "    \"客户经理每年评聘申报时间是怎样的？\",\n",
    "    \"客户经理的考核标准是什么？\"\n",
    "]\n",
    "\n",
    "# 处理每个查询\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"查询: {query}\")\n",
    "    \n",
    "    # 使用MultiQueryRetriever处理查询\n",
    "    response, unique_pages = process_query_with_multi_retriever(\n",
    "        query, \n",
    "        multi_retriever, \n",
    "        llm\n",
    "    )\n",
    "    \n",
    "    # 打印回答\n",
    "    print(\"\\n回答:\")\n",
    "    print(response[\"output_text\"])\n",
    "    \n",
    "    # 打印来源页码\n",
    "    print(\"\\n来源页码:\")\n",
    "    for page in sorted(unique_pages):\n",
    "        print(f\"- 第 {page} 页\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
